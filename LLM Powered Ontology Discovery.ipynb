{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf53103",
   "metadata": {},
   "source": [
    "# Ontology Discovery with NER and Relation Extraction\n",
    "\n",
    "This notebook demonstrates an automated approach to ontology discovery by combining:\n",
    "1. **Named Entity Recognition (NER)** - Identifying entities in text using BERT\n",
    "2. **Relation Extraction** - Using LLMs to discover relationships between entities\n",
    "\n",
    "The workflow:\n",
    "- Extract entities from text using a pre-trained NER model\n",
    "- Use an LLM to identify meaningful relationships between entities\n",
    "- Structure the output in an ontology-friendly format (entities + relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017876a",
   "metadata": {},
   "source": [
    "## Step 1: Named Entity Recognition (NER)\n",
    "\n",
    "In this step, we:\n",
    "1. Load a pre-trained BERT model fine-tuned for Named Entity Recognition\n",
    "2. Process example text to identify entities (Person, Location, Organization, etc.)\n",
    "3. Map the BIO tags (B-PER, I-PER, etc.) to human-readable entity types\n",
    "\n",
    "**Model Used**: `dslim/bert-base-NER` - A BERT model trained on the CoNLL-2003 dataset\n",
    "\n",
    "**Entity Types Detected**:\n",
    "- **Person**: Individual names\n",
    "- **Location**: Geographic locations (cities, countries, etc.)\n",
    "- **Organization**: Companies, institutions, agencies\n",
    "- **Miscellaneous**: Other named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fb3b20-d896-4c4c-9f4b-091840a3eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'Person', 'score': np.float32(0.9990139), 'word': 'Wolfgang'},\n",
       " {'entity': 'Location', 'score': np.float32(0.999645), 'word': 'Berlin'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "entity_mapping = {\n",
    "    'B-PER': 'Person',\n",
    "    'I-PER': 'Person',\n",
    "    'B-ORG': 'Organization',\n",
    "    'I-ORG': 'Organization',\n",
    "    'B-LOC': 'Location',\n",
    "    'I-LOC': 'Location',\n",
    "    'B-MISC': 'Miscellaneous',\n",
    "    'I-MISC': 'Miscellaneous'\n",
    "}\n",
    "\n",
    "filtered_entities = [\n",
    "    {\n",
    "        'entity': entity_mapping.get(result['entity'], result['entity']),\n",
    "        'score': result['score'],\n",
    "        'word': result['word']\n",
    "    }\n",
    "    for result in ner_results\n",
    "]\n",
    "\n",
    "filtered_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced36b7",
   "metadata": {},
   "source": [
    "## Step 2: Relation Extraction with LLM\n",
    "\n",
    "Now we use a Large Language Model (LLM) to discover relationships between the identified entities.\n",
    "\n",
    "**Approach**:\n",
    "- Send the original text and NER results to an LLM (Gemma3 via Ollama)\n",
    "- The LLM is prompted to act as an ontology expert\n",
    "- It extracts relationships ONLY between existing entities (no hallucination)\n",
    "- Output is structured as JSON with entities and relations\n",
    "\n",
    "**Output Format**:\n",
    "```json\n",
    "{\n",
    "  \"entities\": [{\"id\": \"...\", \"type\": \"...\"}],\n",
    "  \"relations\": [{\"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Constraints**:\n",
    "- Predicates use camelCase (e.g., `livesIn`, `worksFor`)\n",
    "- Entity types follow ontology classes (Person, Location, Organization)\n",
    "- Relations are expressed as RDF-style triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa27dbb6-4b09-463b-b4b7-fac920084608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'id': 'Wolfgang', 'type': 'Person'},\n",
       "  {'id': 'Berlin', 'type': 'Location'}],\n",
       " 'relations': [{'subject': 'Wolfgang',\n",
       "   'predicate': 'livesIn',\n",
       "   'object': 'Berlin'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=\"gemma3\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "                    You are an expert in ontology discovery and relation extraction.\n",
    "\n",
    "                    Given:\n",
    "                    1. Input text\n",
    "                    2. A list of identified entities with their types\n",
    "\n",
    "                    Your task:\n",
    "                    - Identify relationships ONLY between the given entities.\n",
    "                    - Do NOT invent new entities.\n",
    "                    - Use ontology-friendly predicate names (camelCase, verbs).\n",
    "\n",
    "                    Output JSON with TWO sections:\n",
    "                    1. \"entities\": list of entities with fields:\n",
    "                    - \"id\": canonical name\n",
    "                    - \"type\": ontology class (Person, Location, Organization, etc.)\n",
    "\n",
    "                    2. \"relations\": list of triples with fields:\n",
    "                    - \"subject\"\n",
    "                    - \"predicate\"\n",
    "                    - \"object\"\n",
    "\n",
    "                    Rules:\n",
    "                    - Use consistent ontology classes.\n",
    "                    - Predicates must be reusable OWL ObjectProperties.\n",
    "                    - Output ONLY valid JSON.\n",
    "                    \"\"\"),\n",
    "    HumanMessage(content=f\"\"\"\n",
    "                    Original text: {example}\n",
    "\n",
    "                    Identified entities: {ner_results}\n",
    "\n",
    "                    Extract the relationships between these entities based on the original text.\n",
    "                    \"\"\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "json_str = response.strip()[7:-3].strip()\n",
    "llm_output = json.loads(json_str)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0681ce0",
   "metadata": {},
   "source": [
    "## Workflow Summary: End-to-End Ontology Discovery Pipeline\n",
    "\n",
    "This notebook demonstrates a complete pipeline from raw text to a queryable knowledge graph:\n",
    "\n",
    "**Phase 1: Entity & Relation Extraction (Steps 1-2)**\n",
    "- NER model identifies entities (Person, Location, Organization)\n",
    "- LLM extracts meaningful relationships between entities\n",
    "- Structured JSON output with typed entities and predicates\n",
    "\n",
    "**Phase 2: Ontology Population (Steps 3-7)**\n",
    "- Load pre-defined ontology schema from Protege\n",
    "- Map extracted data to OWL classes and properties\n",
    "- Create individuals and establish relationships\n",
    "- Validate logical consistency with reasoner\n",
    "\n",
    "**Phase 3: Graph Database Storage (Steps 8-15)**\n",
    "- Extract ontology data (entities + relationships)\n",
    "- Connect to Neo4j graph database\n",
    "- Import nodes with semantic labels\n",
    "- Create typed relationships for graph traversal\n",
    "\n",
    "**Result**: A validated, scalable knowledge graph ready for complex queries, reasoning, and integration with downstream applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18a275",
   "metadata": {},
   "source": [
    "## Step 3: Loading Pre-existing Ontology from Protege\n",
    "\n",
    "In this step, we load an ontology that was created and validated in Protege. This ontology contains the class hierarchy and property definitions that will be used to structure our extracted entities.\n",
    "\n",
    "**Key Components**:\n",
    "- **Owlready2**: Python library for manipulating OWL 2.0 ontologies\n",
    "- **Ontology File**: Test1.rdf - created in Protege with predefined classes and properties\n",
    "- **Classes**: Person, Location, Organization, Event\n",
    "- **Object Properties**: livesIn, bornIn, worksFor, etc.\n",
    "\n",
    "The loaded ontology provides the schema/structure that our NER-extracted entities will conform to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2335b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.Entity\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.Event\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.Location\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.Organisation\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.Person\n",
      "\n",
      "Object Properties:\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.livesIn\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.locatedIn\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.participatedIn\n",
      "C:\\Users\\zohai\\OneDrive\\Documents\\Test1.worksFor\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "from owlready2.reasoning import sync_reasoner\n",
    "\n",
    "onto = get_ontology(\"file://C:\\\\Users\\\\zohai\\\\OneDrive\\\\Documents\\\\Test1.rdf\").load()\n",
    "print(\"Classes:\")\n",
    "for c in onto.classes():\n",
    "    print(c)\n",
    "\n",
    "print(\"\\nObject Properties:\")\n",
    "for p in onto.object_properties():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee6645",
   "metadata": {},
   "source": [
    "## Step 4: Mapping NER Results to Ontology Schema\n",
    "\n",
    "To bridge the gap between raw NER output and formal ontology, we create mapping dictionaries:\n",
    "\n",
    "**CLASS_MAP**: Maps natural language entity types to OWL classes\n",
    "- \"person\", \"human\" → `onto.Person`\n",
    "- \"location\", \"place\" → `onto.Location`\n",
    "- \"organization\" → `onto.Organization`\n",
    "\n",
    "**PROPERTY_MAP**: Maps relation predicates to OWL object properties\n",
    "- \"livesIn\", \"residesIn\" → `onto.livesIn`\n",
    "- \"bornIn\" → `onto.bornIn`\n",
    "- \"worksFor\" → `onto.worksFor`\n",
    "\n",
    "This mapping ensures consistent ontology population regardless of variations in LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bbb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAP = {\n",
    "    \"person\": onto.Person,\n",
    "    \"human\": onto.Person,\n",
    "    \"location\": onto.Location,\n",
    "    \"place\": onto.Location,\n",
    "    \"organization\": onto.Organization,\n",
    "    \"event\": onto.Event,\n",
    "}\n",
    "PROPERTY_MAP = {\n",
    "    \"livesin\": onto.livesIn,\n",
    "    \"residesin\": onto.livesIn,\n",
    "    \"bornin\": onto.bornIn,\n",
    "    \"worksfor\": onto.worksFor,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89f1bd",
   "metadata": {},
   "source": [
    "## Step 5: Creating Ontology Individuals\n",
    "\n",
    "Now we instantiate individuals (instances) in the ontology based on the entities extracted by NER and validated by the LLM.\n",
    "\n",
    "**Process**:\n",
    "1. Iterate through entities from LLM output\n",
    "2. Map each entity type to the appropriate OWL class using CLASS_MAP\n",
    "3. Create new individuals in the ontology\n",
    "4. Cache individuals for later use in relationship creation\n",
    "\n",
    "**Result**: Ontology populated with concrete instances (e.g., \"Wolfgang\" as an instance of Person class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8271401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_cache = {}\n",
    "with onto:\n",
    "    for ent in llm_output[\"entities\"]:\n",
    "        name = ent[\"id\"]\n",
    "        ent_type = ent[\"type\"].lower()\n",
    "\n",
    "        cls = CLASS_MAP.get(ent_type)\n",
    "        if cls is None:\n",
    "            raise ValueError(f\"Unknown entity type: {ent['type']}\")\n",
    "\n",
    "        if name not in entity_cache:\n",
    "            entity_cache[name] = cls(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006f903",
   "metadata": {},
   "source": [
    "## Step 6: Adding Relationships Between Individuals\n",
    "\n",
    "With individuals created, we now establish relationships between them using object properties.\n",
    "\n",
    "**Process**:\n",
    "1. Iterate through relations from LLM output\n",
    "2. Retrieve subject and object individuals from the entity cache\n",
    "3. Map predicates to OWL properties using PROPERTY_MAP\n",
    "4. Establish property assertions (e.g., `Wolfgang.livesIn.append(Berlin)`)\n",
    "\n",
    "**Result**: A fully connected knowledge graph with typed relationships conforming to the ontology schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ac6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in llm_output[\"relations\"]:\n",
    "    subj = entity_cache.get(rel[\"subject\"])\n",
    "    obj = entity_cache.get(rel[\"object\"])\n",
    "\n",
    "    pred = rel[\"predicate\"].lower()\n",
    "    prop = PROPERTY_MAP.get(pred)\n",
    "\n",
    "    if prop is None:\n",
    "        raise ValueError(f\"Unknown relation: {rel['predicate']}\")\n",
    "\n",
    "    prop[subj].append(obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e4111",
   "metadata": {},
   "source": [
    "## Step 7: Ontology Validation with Reasoner\n",
    "\n",
    "Before deploying the ontology, we validate it using a reasoning engine to ensure logical consistency.\n",
    "\n",
    "**Reasoner**: HermiT (via Owlready2's `sync_reasoner`)\n",
    "\n",
    "**What it checks**:\n",
    "- Class satisfiability (no contradictory class definitions)\n",
    "- Property domain/range consistency\n",
    "- Cardinality constraints\n",
    "- Disjointness axioms\n",
    "\n",
    "**Output**: ✅ if consistent, ❌ if inconsistent (with error details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c7125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx2000M -cp c:\\Users\\zohai\\University\\Ontology Discovery\\.venv\\Lib\\site-packages\\owlready2\\hermit;c:\\Users\\zohai\\University\\Ontology Discovery\\.venv\\Lib\\site-packages\\owlready2\\hermit\\HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:///C:/Users/zohai/AppData/Local/Temp/tmp3318ot20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ontology is consistent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * HermiT took 0.365386962890625 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sync_reasoner()\n",
    "    print(\"✅ Ontology is consistent\")\n",
    "except OwlReadyInconsistentOntologyError:\n",
    "    print(\"❌ Ontology is inconsistent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd1471",
   "metadata": {},
   "source": [
    "## Step 8: Inspecting Ontology Content\n",
    "\n",
    "Explore the populated ontology to verify individuals and their class memberships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e1eab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual: Berlin\n",
      "  Types: ['Location']\n",
      "Individual: OpenAI\n",
      "  Types: ['Organisation']\n",
      "Individual: Wolfgang\n",
      "  Types: ['Person']\n"
     ]
    }
   ],
   "source": [
    "for ind in onto.individuals():\n",
    "    print(\"Individual:\", ind.name)\n",
    "    print(\"  Types:\", [cls.name for cls in ind.is_a])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec920b8",
   "metadata": {},
   "source": [
    "## Step 9: Visualizing Relationships\n",
    "\n",
    "Display all relationships in human-readable triple format (subject → predicate → object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a49f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI --locatedIn--> Berlin\n",
      "Wolfgang --livesIn--> Berlin\n",
      "Wolfgang --livesIn--> Berlin\n",
      "Wolfgang --worksFor--> OpenAI\n"
     ]
    }
   ],
   "source": [
    "for ind in onto.individuals():\n",
    "    for prop in ind.get_properties():\n",
    "        for value in prop[ind]:\n",
    "            print(f\"{ind.name} --{prop.name}--> {value.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71e529",
   "metadata": {},
   "source": [
    "## Step 10: Extracting Data for Neo4j\n",
    "\n",
    "Prepare ontology data for export to a graph database. This involves:\n",
    "\n",
    "**Entity Types Extraction**: Create a dictionary mapping each individual to its classes for Neo4j node labels.\n",
    "\n",
    "**Relations Extraction**: Convert OWL property assertions into a list of relationship dictionaries suitable for Neo4j import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcd63d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Berlin': ['Location'], 'OpenAI': ['Organisation'], 'Wolfgang': ['Person']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_types = {}\n",
    "\n",
    "for ind in onto.individuals():\n",
    "    entity_types[ind.name] = [\n",
    "        cls.name for cls in ind.is_a\n",
    "        if hasattr(cls, \"name\")\n",
    "    ]\n",
    "entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b8530d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject': 'OpenAI', 'predicate': 'locatedIn', 'object': 'Berlin'},\n",
       " {'subject': 'Wolfgang', 'predicate': 'livesIn', 'object': 'Berlin'},\n",
       " {'subject': 'Wolfgang', 'predicate': 'livesIn', 'object': 'Berlin'},\n",
       " {'subject': 'Wolfgang', 'predicate': 'worksFor', 'object': 'OpenAI'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = []\n",
    "\n",
    "for ind in onto.individuals():\n",
    "    for prop in ind.get_properties():\n",
    "        for value in prop[ind]:\n",
    "            relations.append({\n",
    "                \"subject\": ind.name,\n",
    "                \"predicate\": prop.name,\n",
    "                \"object\": value.name\n",
    "            })\n",
    "relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ca21b",
   "metadata": {},
   "source": [
    "## Step 11: Connecting to Neo4j Graph Database\n",
    "\n",
    "Establish connection to Neo4j using the official Python driver.\n",
    "\n",
    "**Configuration**:\n",
    "- **Database**: Neo4j running locally on port 7687\n",
    "- **Authentication**: Using credentials stored in environment variables (`.env` file)\n",
    "- **Driver**: `neo4j-driver` for executing Cypher queries\n",
    "\n",
    "Neo4j will serve as the scalable storage backend for querying and traversing the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d305708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "load_dotenv()\n",
    "driver = GraphDatabase.driver(\n",
    "    \"neo4j://127.0.0.1:7687\",\n",
    "    auth=(\"neo4j\", os.getenv(\"NEO4J_PASSWORD\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d286c1",
   "metadata": {},
   "source": [
    "## Step 12: Defining Entity Creation Function\n",
    "\n",
    "Create a helper function to insert entities into Neo4j as nodes with appropriate labels.\n",
    "\n",
    "**Key Features**:\n",
    "- Uses `MERGE` to avoid duplicate nodes\n",
    "- Applies multiple labels based on ontology class hierarchy\n",
    "- Labels reflect the entity's types (e.g., a node could be both Person and NamedIndividual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40342d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity(tx, entity_id, labels):\n",
    "    label_str = \":\" + \":\".join(labels)\n",
    "    tx.run(\n",
    "        f\"MERGE (e{label_str} {{id: $id}})\",\n",
    "        id=entity_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff689d",
   "metadata": {},
   "source": [
    "## Step 13: Importing Entities into Neo4j\n",
    "\n",
    "Execute the entity creation for all individuals from the ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf75942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    for entity, labels in entity_types.items():\n",
    "        session.execute_write(create_entity, entity, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0965e8",
   "metadata": {},
   "source": [
    "## Step 14: Defining Relationship Creation Function\n",
    "\n",
    "Create a helper function to insert relationships between nodes as Neo4j edges.\n",
    "\n",
    "**Features**:\n",
    "- Matches both source and target nodes by ID\n",
    "- Creates directed relationships using predicate as relationship type\n",
    "- Uses uppercase for relationship types (Neo4j convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f55fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relation(tx, s, p, o):\n",
    "    tx.run(\n",
    "        f\"\"\"\n",
    "        MATCH (a {{id: $s}})\n",
    "        MATCH (b {{id: $o}})\n",
    "        MERGE (a)-[:{p.upper()}]->(b)\n",
    "        \"\"\",\n",
    "        s=s,\n",
    "        o=o\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623b8bd",
   "metadata": {},
   "source": [
    "## Step 15: Importing Relationships into Neo4j\n",
    "\n",
    "Execute the relationship creation for all property assertions from the ontology.\n",
    "\n",
    "**Result**: A complete knowledge graph stored in Neo4j, ready for:\n",
    "- Complex graph queries using Cypher\n",
    "- Relationship traversal and pattern matching\n",
    "- Graph analytics and visualization\n",
    "- Integration with downstream applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82f77b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    for r in relations:\n",
    "        session.execute_write(\n",
    "            create_relation,\n",
    "            r[\"subject\"],\n",
    "            r[\"predicate\"],\n",
    "            r[\"object\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47abd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
